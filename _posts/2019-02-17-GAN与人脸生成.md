---
layout:     post
title:      "PaperReading-GAN"
subtitle:   " \"利用GAN生成高清人脸\""
date:       2019-02-17 22:02:00
author:     "HeShuai"
header-img: "img/banner/post-default.jpg"
catalog: true
tags:
    - GAN
    - Paper Reading
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


## 1. [GAN]

**Training Scheme**

![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191206151343.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191206151247.png)


**Mesure of GAN**
GAN的loss不能反映GAN的最终结果，最后评估generator的结果应该是其生成内容的质量和方差。评估指标有两种IS和FID，大致都是借用Iception的中间特征来计算。[click me](https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732)

## 2. [Style Gan](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431)

**style gan网络结构**
相对于传统的generator，style gan提出的模型结果采用constant tensor输入的形式，引入变换tensor$w$实现对不同卷积层（原论文中有三层维度）的变化控制。该结果提供了两种可能：
1. 实现了特征的一定程度上的可线性分离。
2. 不同分辨率上的特征修改提供了点对点特征修改的可能和手段。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191128143906.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191125155752.png)

**随机引入量**
在style gan的基础上进一步引入随机变换，实验证明可能极大提升生成结果的细节和逼真程度。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191125160544.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191125160700.png)

**style mixing**
原只是为了解决模型认为不同分辨率的特征修改之间存在依赖关系，随机从两个向量中选取进行特征修改。提供了一种多图合成的可能，如利用A的头发和B的性别，mix出一个结果。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191125160948.png)

**Truncation trick in W**
让$w$更靠近平均特征所得到的平均向量，并引入常量控制靠近的距离或者幅度。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191125161910.png)

**implementation details**
1. upsample/downsample
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191206101838.png)
2. Equalized Learning Rate for all layers

## 3. [StyleGan2](https://arxiv.org/pdf/1912.04958.pdf)
artifact的移除，新的progressive training方法，PPL reguilization。

**Weight Demodulation**

作者认为sgan1中的artifact是AdaIN造成的，因此重新设计了一种normalization方法。

- 该模块的设计基于一个假设，输入为0/1正太分布。实际上是一种weak normalization，因为基于静态假设而不是实际的输入分布（但是验证可行）
- 由于基础假设，不需要处理均值问题，只需要归一化方差。
- 归一化方差论文提出一种更加简洁高效的方法，通过改变conv对应权重来取得相同效果。
- [实现](#stylegan2-modualation)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217135925.png)

**Path Length regularization**

作者发现FID的关注重点是纹理而不是形状等，相反path length能描述形态的合理性。因此想办法在训练阶段加入PL Regularization。

- lazy regularization: loss_sum = loss + reg，事实上reg不用每此都算致。以此应对PL计算量大的问题。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217143023.png)
- 加入PL regularization可以降低生成诡异图的几率
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217144122.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217144004.png)

**Progressive growing revisited**

作者发现progressive growing会导致一些细节变化不够平滑，如眼睛/牙齿等并没有随着脸的朝向变化变而变。

- 模型结构改进，增加residual connection和input/output skips。带来的提升很明显。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217145809.png)
- 通过上述的input/output skips，可取得跟pp训练模式一致的效果：训练初期低分辨率贡献最大，往后高分辨率贡献上升。但是作者发现后期高分辨率的贡献不够，因此增加了高分辨率部分的通道数（加宽而不是加深）。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200217151603.png)



## 4. [progressive growing of gans](https://arxiv.org/abs/1710.10196)

提供了一种分辨率渐进式的训练方式，从4X4开始，逐渐增加模型层数和输出分辨率，最终输出1024X1024的分辨率。打破了传统一步到位的GAN不能生成高分辨率结果的难题，并且降低了model collapse的几率和降低训练时间（尽管还是很长，8v100两周）。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191128174758.png)



## 5. Growing GANs

1. **growing structure**
从一层4 $\times$ 4分辨率的结构开始，渐进地增加层数。
2. **fade in**
每次增加层地时候，前面层不会被冷却，引入fade in机制组件让高分辨率输出替换原低分辨率的结果。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191128175411.png)
3. **pixel normalization**
每层卷积之后不用BN而用PN
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191128175640.png)
4. **minibatch standare deviation**
传统的GAN生成结果方差（样本内方差pixel level）要比真实数据要小。在discriminator尾部计算minibatch的pixel方差，作为独立的一维特征，辅助D判断这个生成结果是否是真的
5. **Equalized Learning Rate**
为使每个层的训练速度大致，利用层参数量进行weights均衡化。（没搞懂）
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191128180428.png)

## 6. [Image2StyleGAN](https://arxiv.org/pdf/1904.03189.pdf)

## Code

### <span id='stylegan2-modualation'> stylegan2 modualation </span>

```python
    def modulated_conv2d_layer_training(x, y, fmaps, kernel, up=False, down=False, demodulate=True,
                                        resample_kernel=None, gain=1,
                                        use_wscale=True, lrmul=1, fused_modconv=True, weight_var='weight',
                                        mod_weight_var='mod_weight', mod_bias_var='mod_bias', impl='cuda'):
        assert not (up and down)
        assert kernel >= 1 and kernel % 2 == 1

        # Get weight.
        w = get_weight([kernel, kernel, x.shape[1].value, fmaps], gain=gain, use_wscale=use_wscale, lrmul=lrmul,
                       weight_var=weight_var)
        ww = w[np.newaxis]  # [BkkIO] Introduce minibatch dimension.

        # Modulate.
        s = dense_layer(y, fmaps=x.shape[1].value, weight_var=mod_weight_var)  # [BI] Transform incoming W to style.
        s = apply_bias_act(s, bias_var=mod_bias_var, impl=impl) + 1  # [BI] Add bias (initially 1).
        ww *= tf.cast(s[:, np.newaxis, np.newaxis, :, np.newaxis], w.dtype)  # [BkkIO] Scale input feature maps.

        # Demodulate.
        if demodulate:
            d = tf.rsqrt(tf.reduce_sum(tf.square(ww), axis=[1, 2, 3]) + 1e-8)  # [BO] Scaling factor.
            ww *= d[:, np.newaxis, np.newaxis, np.newaxis, :]  # [BkkIO] Scale output feature maps.

        # Reshape/scale input.
        if fused_modconv:
            x = tf.reshape(x, [1, -1, x.shape[2], x.shape[3]])  # Fused => reshape minibatch to convolution groups.
            w = tf.reshape(tf.transpose(ww, [1, 2, 3, 0, 4]), [ww.shape[1], ww.shape[2], ww.shape[3], -1])
        else:
            x *= tf.cast(s[:, :, np.newaxis, np.newaxis], x.dtype)  # [BIhw] Not fused => scale input activations.

        # Convolution with optional up/downsampling.
        if up:
            x = upsample_conv_2d(x, tf.cast(w, x.dtype), data_format='NCHW', k=resample_kernel, impl=impl)
        elif down:
            x = conv_downsample_2d(x, tf.cast(w, x.dtype), data_format='NCHW', k=resample_kernel, impl=impl)
        else:
            x = tf.nn.conv2d(x, tf.cast(w, x.dtype), data_format='NCHW', strides=[1, 1, 1, 1], padding='SAME')

        # Reshape/scale output.
        if fused_modconv:
            x = tf.reshape(x, [-1, fmaps, x.shape[2],
                               x.shape[3]])  # Fused => reshape convolution groups back to minibatch.
        elif demodulate:
            x *= tf.cast(d[:, :, np.newaxis, np.newaxis], x.dtype)  # [BOhw] Not fused => scale output activations.
        return x
```

