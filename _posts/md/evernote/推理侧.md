[TOC]
# 推理侧

## CPU or GPU

### CPU与GPU的异同



### Inference on CPU

EIGEN

MKL-DNN

https://arxiv.org/pdf/1809.02697.pdf 文章提出一种op与图层次的CPU推理优化方法，采用TVM，据说能得到比openvino更牛的结果。具体方法没看，大概就是针对CPU的硬件情况进行针对性优化，并且借助TVM采取搜索优化的形式。

https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37631.pdf GOOGLE的一篇文章，利用CPU SSSE3/SSE4和其他的一些方法进行CPU推理加速。内容比较细，甚至附带code说明。

### Inference on GPU

tensorcore

## 推理框架

### 快餐总结

| 框架     | 主要特点 | 适用平台和设备 | 优点 | 缺点 |
| -------- | -------- | -------------- | ---- | ---- |
| Openvino |          |                |      |      |
| TVM      |          |                |      |      |

### [ONNX](https://github.com/onnx/tutorials)

坑：
1. 尽量常量折叠（减少op数目），转的时候可以跳过很多坑。


### [OpenVINO](https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html#Install-GPU)
Intel出品的针对Intel硬件进行优化的编译器和推理框架。
主要特点：
1. 支持大多主流训练框架：tensorflow\pytorch
2. 支持量化 
3. 支持windows
4. 只能支持intel的硬件设备，包括CPU/GPU集显/VPU/FPGA，CPU supports FP32 and Int8 while its GPU supports FP16 and FP32.
5. IR转出来的模型可以加载于cv.dnn中
6. CPU模式资源占用较高，采用intel PBB作为多进程引擎。

---
坑：
1. openvino调用dll错误
[配置永久环境变量](https://www.intel.com/content/www/us/en/support/articles/000033440/boards-and-kits/neural-compute-sticks.html)，同时如果是pycharm+conda还需要配置pythonpath变量
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/openvino_pypath.png)
2. IE调用时报算子不支持，需要增加cpu extention
[增加预编译的cpu.dll]('C:\Intel\computer_vision_sdk_2018.5.445\deployment_tools\inference_engine\bin\intel64\Release\cpu_extension_avx2.dll')
3. ie初始化错误，plugin.xml也需要放置于release目录下


### [TVM](https://arxiv.org/pdf/1802.04799.pdf)
TVM的最主要的特点是支持广泛，几乎囊括常见的设备和架构。为了满足广泛支持的特点并最大化优化性能，采用了AUTO-TVM进行针对性的编译性能优化。也因此改特点，TVM可以很方便地针对不同的设备和架构进行实际的性能评估和优化。
TVM Stack主要由两大块组成(如下图所示)。其中compilation stack负责模型编译和优化，runtime stack则负责部署。两个部分在独立的硬件个体中运行。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/TVM_stack.png)
主要特点：
1. 支持主流训练框架：tensorflow/keras/pytorch
2. 全平台全设备支持：CPU/GPU/Linux/Windows/Mobile/FPGA
3. 从Graph和op两个级别进行性能优化。为适配不同的底层硬件设备，采用learning base(随机数)的方法来增加编译(从ops到low-level code)性能。
![](https://raw.githubusercontent.com/mightycatty/mightycatty.github.io/master/images/tvm_structure.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/TVM-paper-note.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/openvino_precision.png)

### [TensorRT]
黄老板家的针对N卡的推理框架，整体与openvino类似，主要由model optimizer和runtime组成。
>大概步骤：
>1. 模型转换与构建，从其他框架的图解析出结果，构建一个Tensorrt network。支持的源图格式有pb/uff/onnx/pytorch等。其中tf pb需要中间先转uff格式或者onnx格式。这一步坑最多，算子不支持或者解析构造模型那一步都会报错。一般建议是都转成onnx中间格式。
>2. engine buiding。1所得的图进行针对模型和显卡型号的算子搜索和优化，优化层次较1更低一层。该部分耗时会较长，但是结果可以serialized出去，下次直接复用。
>3. runtime inference

主要特点：
1. 针对N卡，tensorcore加速。
2. 支持fp32/16还有int8
3. 支持与tf混跑，即不支持的算子跑tf支持的跑tensorrt。这部分甚至整合入tensorflow api，只需要很少的代码量即可实现。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191202112254.png)
Model Optimizer:
主要是一些常规的图层级的优化和针对硬件、显卡型号、操作系统的底层算子优化。值得注意的是：
> The generated plan files are not portable across platforms or TensorRT versions. Plans are specific to the exact GPU model they were built on (in addition to the platforms and the TensorRT version) and must be re-targeted to the specific GPU in case you want to run them on a different GPU.
> ![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191202112517.png)

Helpful resource:
1. [tensorrt wiki](
https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/infer/Core/Builder.html)
2. [official document](
https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#add_custom_layer_python)
3. [official pythonAPI sample](
https://github.com/NVIDIA/object-detection-tensorrt-example)
4. [intergration with tensorflow](
https://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe)

---
 坑：
1. Reshape报"cannot reshape with a different volume"错误。
~~[一说法是不能reshape大于四维](https://devtalk.nvidia.com/default/topic/1043494/tensorrt/tensorrt4-reshape-volume-mismatch/)~~
貌似tensorrt下所有的操作都是默认是4维的tensor，如果不足就会补，超过好像就会有问题。
2. tensor broadcast机制不如tf
如X（512， 4， 4） * Y（512， 1， 1） 会报Invalid scale mode, nbWeights: 8192
3. transpose broken(uff parser)
transpose在uff parser阶段似乎没有任何作用[](https://devtalk.nvidia.com/default/topic/1026481/transpose-does-not-work-in-tensorrt/)
```python
style_w = tf.strided_slice(style, [0, 0, 0, 0], [1, x.shape[1].value, 1, 1]) + 1
style_b = tf.strided_slice(style, [0, x.shape[1].value, 0, 0,[1, x.shape[1].value*2, 1, 1])
# transpose
style_w = nchw_to_nhwc(style_w)
style_b = nchw_to_nhwc(style_b)
x = nchw_to_nhwc(x)
y = x * style_w + style_b
y = nhwc_to_nchw(y)


[TensorRT] VERBOSE: UFFParser: generator_fix/4x4/Const/StyleMod/add_1 -> [512,1,1]
[TensorRT] VERBOSE: UFFParser: Applying order forwarding to: generator_fix/4x4/Const/StyleMod/add_1
[TensorRT] VERBOSE: UFFParser: Parsing generator_fix/4x4/Const/StyleMod/transpose[Op: Transpose]. Inputs: generator_fix/4x4/Const/StyleMod/add_1
[TensorRT] VERBOSE: UFFParser: generator_fix/4x4/Const/StyleMod/transpose -> [512,1,1]
```
4. 貌似不能对batch size维度进行操作，如transpose(1, 2, 3, 0)
5. 转之前进行常量折叠和图层次的算子融合，可以降低转来转去的的过程中跳坑的几率。
6. network构建的时候貌似可以关闭implicit batch size，那么在onnx转trt network的时候就不会默认所有操作都增加一维，还可能可以操作bn维度（未验证）。
7. trt和tf混合使用时，如果trt在tf前初始化trt会报runtime错误。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200102160328.png)

### [MMN]