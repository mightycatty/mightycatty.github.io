# 调参

### 1. [The higher lr, the better](https://medium.com/@davidlmorton/exploring-the-learning-rate-survey-3e328d17cf79)

采用线性增长的方式探针模型的初始learning rate设定，个人认为是一种寻找当前任务数据下能稳定收敛的初始learning rate。文章还试图说明越大的learning rate对最终acc和训练e'poch更好。

![1571895451807](C:\Users\heshuai\AppData\Roaming\Typora\typora-user-images\1571895451807.png)

![1571895479865](C:\Users\heshuai\AppData\Roaming\Typora\typora-user-images\1571895479865.png)

有一个疑问是，这只是寻找初始learning rate但是后面的decay是否会涉及以及是否会对结果产生影响。

### 2. [**Increase Batch Size instead of decrease lr**](https://arxiv.org/pdf/1711.00489.pdf)

论文宣称训练过程中增加batch size可以获得与降低lr一样的效果，但是收敛epoch可以大幅度降低。疑问是batch size增大单步时间也长，那总体训练时间上是否缩短了？

### 3. [lr for various opt methods](https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2)

文章主关于lr，opt，训练时间，精确率和模型大小之间的关系。

![1573546577632](C:\Users\heshuai\AppData\Roaming\Typora\typora-user-images\1573546577632.png)

1. 不同的opt method有不同的适用lr范围。
2. adam和momentum有着更大的lr适用带宽。
3. 似乎adam的学习率比momentum要小的多。
4. RMSProp表现最垃圾。

![1573546775368](C:\Users\heshuai\AppData\Roaming\Typora\typora-user-images\1573546775368.png)

![1573546871215](C:\Users\heshuai\AppData\Roaming\Typora\typora-user-images\1573546871215.png)

1. 在validation上，adam收敛最稳定最快，其次adadelta和momentum。

2. 同一个搜索到的最有lr和opt，model size与训练时间成很完美的线性关系。似乎预示着lr和opt的选择与模型大小无关（与数据有关？？）

   