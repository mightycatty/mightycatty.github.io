---
layout:     post
title:      "推理框架"
subtitle:   " \"深度学习推理框架\""
date:       2020-02-16 01:01:00
author:     "HeShuai"
header-img: "img/banner/dataflow.jpg"
catalog: true
tags:
    - 推理框架
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 推理框架

### 总结

| 框架         | 主要特点                 | 适用平台和设备                    | 优点           | 缺点                            | 量化                                        | 算子支持程度          |
| ------------ | ------------------------ | --------------------------------- | -------------- | ------------------------------- | ------------------------------------------- | --------------------- |
| Openvino     | 主攻CPU推理              | windows、Intel I系列CPU、集成显卡 | CPU推理、快    | 默认资源占用太高                | 截至2019 R3，量化工具并不好用，没有代码demo | 较好                  |
| TVM          | 自动针对硬件特性优化编译 | 全垒打                            | 全垒打         | 相对不是很成熟                  | 暂未尝试                                    | -                     |
| Tensorrt     | 针对N卡 GPU推理          | windows、linux、GPU               | GPU推理        | 只能GPU推理、加速效果因模型而异 | 量化简单                                    | 一般（7.0有较大改善） |
| MNN          | 移动端推理               | 移动端、Android、IOS              |                |                                 |                                             |                       |
| NCNN         | 移动端                   | 移动端、Android、IOS              |                |                                 |                                             |                       |
| TF-LITE      | 移动端                   | Android                           | tensorflow御用 |                                 |                                             |                       |
| NATIVE-TF    | 严格意义上不是推理框架   | 全垒打                            | 快速测试       | 低效                            | 无                                          | -                     |
| ONNX-RUNTIME |                          |                                   |                |                                 |                                             |                       |




### [OpenVINO](https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html#Install-GPU)
Intel出品的针对Intel硬件进行优化的编译器和推理框架。主推CPU推理，同时支持I系列CPU的集成GPU推理。

主要特点：
1. 支持大多主流训练框架：tensorflow\pytorch
2. 支持量化 
3. 支持windows
4. 只能支持intel的硬件设备，包括CPU/GPU集显/VPU/FPGA，CPU supports FP32 and Int8 while its GPU supports FP16 and FP32.
5. IR转出来的模型可以加载于cv.dnn中
6. CPU模式资源占用较高，采用intel PBB作为多进程引擎。


经验细节：
1. openvino调用dll错误
[配置永久环境变量](https://www.intel.com/content/www/us/en/support/articles/000033440/boards-and-kits/neural-compute-sticks.html)，同时如果是pycharm+conda还需要配置pythonpath变量
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/openvino_pypath.png)
2. IE调用时报算子不支持，需要增加cpu extention
[增加预编译的cpu.dll]('C:\Intel\computer_vision_sdk_2018.5.445\deployment_tools\inference_engine\bin\intel64\Release\cpu_extension_avx2.dll')
3. ie初始化错误，plugin.xml也需要放置于release目录下
4. C+接口可以控制核心数目以控制CPU占用率，python不提供。
5. 尽管只针对I系列CPU优化，AMD-CPU下效率个人尝试也尚可接受。

Python Scrip: 

```python
"""scrip for inference with openvino backend"""
import logging
import os
import sys

from openvino.inference_engine import IENetwork, IECore

CPU_EXTENSION = r'C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\inference_engine\bin\intel64\Release' \
                r'\cpu_extension_avx2.dll'


class InferenceWithOpenvino:
    def __init__(self, model_file,
                 device='CPU',
                 pre_processing_fn=None,
                 post_processing_fn=None,
                 **kwargs):
        self.model_xml = model_file
        self.device = device
        logging.basicConfig(format="[ %(levelname)s ] %(message)s", level=logging.INFO, stream=sys.stdout)
        self.pre_processing_fn = pre_processing_fn
        self.post_processing_fn = post_processing_fn
        self._model_init()

    def _model_init(self):
        model_bin = os.path.splitext(self.model_xml)[0] + ".bin"
        logging.info("Creating Inference Engine")
        ie = IECore()
        found_device = ie.available_devices
        logging.info("found devices:\n{}".format(found_device))
        ie.add_extension(CPU_EXTENSION, "CPU")
        # Read IR
        logging.info("Loading network files:\n\t{}\n\t{}".format(self.model_xml, model_bin))
        net = IENetwork(model=self.model_xml, weights=model_bin)
        self.input_blob = next(iter(net.inputs))
        self.out_blob = next(iter(net.outputs))
        # resize network
        # net.reshape({self.input_blob: (1, 3, 256, 256)})
        if "CPU" in self.device:
            supported_layers = ie.query_network(net, "CPU")
            not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]
            if len(not_supported_layers) != 0:
                logging.error("Following layers are not supported by the plugin for specified device {}:\n {}".
                              format(self.device, ', '.join(not_supported_layers)))
                logging.error(
                    "Please try to specify cpu extensions library path in sample's command line parameters using -l "
                    "or --cpu_extension command line argument")
                sys.exit(1)
        assert len(net.inputs.keys()) == 1, "Sample supports only single input topologgingies"
        assert len(net.outputs) == 1, "Sample supports only single output topologgingies"
        logging.info("Preparing input blobs")
        net.batch_size = 1
        # Loading model to the plugin
        logging.info("Loading model to the plugin")
        config = {}
        # config['CPU_THREADS_NUM'] = '1'
        # config['CLDNN_PLUGIN_PRIORITY'] = '0'
        config = None
        self.exec_net = ie.load_network(network=net, device_name=self.device, config=config)

    def predict(self, input_data):
        if self.pre_processing_fn:
            input_data = self.pre_processing_fn(input_data)
        result = self.sess.run([self.output], feed_dict={self.input: input_data})[0]
        if self.post_processing_fn:
            result = self.post_processing_fn(result)
        return result
```




### [TVM](https://arxiv.org/pdf/1802.04799.pdf)
TVM的最主要的特点是支持广泛，几乎囊括常见的设备和架构。为了满足广泛支持的特点并最大化优化性能，采用了AUTO-TVM进行针对性的编译性能优化。也因此改特点，TVM可以很方便地针对不同的设备和架构进行实际的性能评估和优化。
TVM Stack主要由两大块组成(如下图所示)。其中compilation stack负责模型编译和优化，runtime stack则负责部署。两个部分在独立的硬件个体中运行。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/TVM_stack.png)
主要特点：
1. 支持主流训练框架：tensorflow/keras/pytorch
2. 全平台全设备支持：CPU/GPU/Linux/Windows/Mobile/FPGA
3. 从Graph和op两个级别进行性能优化。为适配不同的底层硬件设备，采用learning base(随机数)的方法来增加编译(从ops到low-level code)性能。
![](https://raw.githubusercontent.com/mightycatty/mightycatty.github.io/master/images/tvm_structure.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/TVM-paper-note.png)
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/openvino_precision.png)

### [TensorRT]

黄老板家的针对N卡的推理框架，整体与openvino类似，主要由model optimizer和runtime组成。
>大概步骤：
>1. 模型转换与构建，从其他框架的图解析出结果，构建一个Tensorrt network。支持的源图格式有pb/uff/onnx/pytorch等。其中tf pb需要中间先转uff格式或者onnx格式。这一步坑最多，算子不支持或者解析构造模型那一步都会报错。一般建议是都转成onnx中间格式。
>2. engine buiding。1所得的图进行针对模型和显卡型号的算子搜索和优化，优化层次较1更低一层。该部分耗时会较长，但是结果可以serialized出去，下次直接复用。
>3. runtime inference

主要特点：

1. 针对N卡，tensorcore加速。
2. 支持fp32/16还有int8
3. 支持与tf混跑，即不支持的算子跑tf支持的跑tensorrt。这部分甚至整合入tensorflow api，只需要很少的代码量即可实现。
![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191202112254.png)
Model Optimizer:
主要是一些常规的图层级的优化和针对硬件、显卡型号、操作系统的底层算子优化。值得注意的是：
> The generated plan files are not portable across platforms or TensorRT versions. Plans are specific to the exact GPU model they were built on (in addition to the platforms and the TensorRT version) and must be re-targeted to the specific GPU in case you want to run them on a different GPU.
> ![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20191202112517.png)

Helpful resource:
1. [tensorrt wiki](
https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/infer/Core/Builder.html)
2. [official document](
https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#add_custom_layer_python)
3. [official pythonAPI sample](
https://github.com/NVIDIA/object-detection-tensorrt-example)
4. [intergration with tensorflow](
https://medium.com/tensorflow/high-performance-inference-with-tensorrt-integration-c4d78795fbfe)


经验细节：
1. Reshape报"cannot reshape with a different volume"错误。
~~[一说法是不能reshape大于四维](https://devtalk.nvidia.com/default/topic/1043494/tensorrt/tensorrt4-reshape-volume-mismatch/)~~
~~貌似tensorrt下所有的操作都是默认是4维的tensor，如果不足就会补，超过好像就会有问题~~。
2. tensor broadcast机制不如tf
如X（512， 4， 4） * Y（512， 1， 1） 会报Invalid scale mode, nbWeights: 8192
3. transpose broken(uff parser)
transpose在uff parser阶段似乎没有任何作用[](https://devtalk.nvidia.com/default/topic/1026481/transpose-does-not-work-in-tensorrt/)
```python
style_w = tf.strided_slice(style, [0, 0, 0, 0], [1, x.shape[1].value, 1, 1]) + 1
style_b = tf.strided_slice(style, [0, x.shape[1].value, 0, 0,[1, x.shape[1].value*2, 1, 1])

style_w = nchw_to_nhwc(style_w)
style_b = nchw_to_nhwc(style_b)
x = nchw_to_nhwc(x)
y = x * style_w + style_b
y = nhwc_to_nchw(y)


[TensorRT] VERBOSE: UFFParser: generator_fix/4x4/Const/StyleMod/add_1 -> [512,1,1]
[TensorRT] VERBOSE: UFFParser: Applying order forwarding to: generator_fix/4x4/Const/StyleMod/add_1
[TensorRT] VERBOSE: UFFParser: Parsing generator_fix/4x4/Const/StyleMod/transpose[Op: Transpose]. Inputs: generator_fix/4x4/Const/StyleMod/add_1
[TensorRT] VERBOSE: UFFParser: generator_fix/4x4/Const/StyleMod/transpose -> [512,1,1]
```
4. ~~貌似不能对batch size维度进行操作，如transpose(1, 2, 3, 0)~~
5. 转之前进行常量折叠和图层次的算子融合，可以降低转来转去的的过程中跳坑的几率。
6. network构建的时候貌似可以关闭implicit batch size，那么在onnx转trt network的时候就不会默认所有操作都增加一维，还可能可以操作bn维度（未验证）。
7. trt和tf混合使用时，如果trt在tf前初始化trt会报runtime错误。
    ![](https://raw.githubusercontent.com/mightycatty/image_bed/master/images/20200102160328.png)
8. 7.0版本之后Tensorrt主攻支持ONNX，所以tensorflow下需要 tf.pb - > onnx - > trt，官方开发人员不推荐使用uff。
9. 加速比不如意的可以查看buil engine的详细，大概找到耗时最高的层，进行针对处理。
10. 加速比不如意很多情况下是卡某个操纵或者算子，算子替换或者查看op fusion，尽可能触发更多的OP fusion（支持的fusion文档可查）。

推理脚本：

```python
"""freestanding testing models with trt backend
tested on tensorrt 7.0.0, might run into issue under other versions.
Known issue:
    1. tensorrt 6.0 onnx parser works with opset<=7
    2. tensorrt 7.0.0 conv2d_transpose is broken(output is different from tf/onnx)
    3. tensorrt 7.0.0 onnx int8-calibrator is (somehow)broken
    4. transpose is broken(data is transposed while shape is kept)
    5. tensorrt 7.0.0 onnx parser only works with networks with explicit batch dim
    6. resize is still not supported for uff parser in tensorrt 7.0.0(though it's claimed)
    7. operation on tensor with over 4dim mostly results in conversion error
Usage Samples:
    1. Build tensorrt IR from .pb or .onnx
Common Issues:
    1. running with remote interpreter in pycharm, add path to environment path
        LD_LIBRARY_PATH=/home/heshuai/application/TensorRT-7.0.0.11/lib
    2. pycuda initialization error
        import pycuda.autoinit
"""

import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)  # disable nasty future warning in tensorflow and numpy

import os
import tensorrt as trt
import logging
import uff
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit
import sys

logging.basicConfig(level=logging.DEBUG,
                    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                    datefmt="%Y-%m-%d %H:%M:%S")
logger = logging.getLogger(__name__)
TRT_LOGGER = trt.Logger(trt.Logger.ERROR)  # global trt logger setting


class TensorrtBuilder:

    @staticmethod
    def _item_to_list(item):
        if not isinstance(item, list):
            if item:
                item = [item]
        return item

    @staticmethod
    def _GiB(val):
        return val * 1 << 30

    @staticmethod
    def _create_optimization_profile(builder, config, input_name, input_shape, batch_size=None):
        """
        required for mode with dynamic shape, call build_engine(network, config) instead of build_cuda_engine(network)
        :param builder:
        :param config:
        :param input_name: name of input nodes
        :param input_shape: ignore batch dim if batch_size is None
        :param batch_size: none for explict batch dim network
        :return: None, alteration is done to 'config' obj
        """
        profile = builder.create_optimization_profile()
        # Fixed explicit batch in input shape
        if batch_size is None:
            batch_size = input_shape[0]
            shape = input_shape[1:]
        # Dynamic explicit batch
        elif input_shape[0] == -1:
            shape = input_shape[1:]
        # Implicit Batch
        else:
            shape = input_shape

        min_batch = batch_size
        opt_batch = batch_size
        max_batch = batch_size
        profile.set_shape(input_name, min=(min_batch, *shape), opt=(opt_batch, *shape), max=(max_batch, *shape))
        config.add_optimization_profile(profile)

    @staticmethod
    def _save_engine(engine, dump_name) -> bool:
        dump_name = '{}.engine'.format(dump_name) if '.engine' not in dump_name else dump_name
        with open(dump_name, 'wb') as f:
            f.write(engine.serialize())
        return True

    @staticmethod
    def _load_engine(trt_runtime, engine_path):
        engine_path = '{}.engine'.format(engine_path) if '.engine' not in engine_path else engine_path
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        engine = trt_runtime.deserialize_cuda_engine(engine_data)
        return engine

    @staticmethod
    def _build_engine(network,
                      builder,
                      explicit_batch_dim=False,
                      max_batch_size=1,
                      max_workspace_size=1 << 30,
                      mix_precision='fp32',
                      calib=None):
        # dynamic shape building config with explict_batch_size = True
        if explicit_batch_dim:
            config = builder.create_builder_config()
            config.max_workspace_size = max_workspace_size
            if mix_precision == 'int8':
                config.set_flag(trt.BuilderFlag.INT8)
                config.int8_calibrator = calib
            if mix_precision == 'fp16':
                config.set_flag(trt.BuilderFlag.FP16)
            input_shape = network.get_input(0).shape
            input_name = network.get_input(0).name
            TensorrtBuilder._create_optimization_profile(builder, config, input_name, input_shape, None)
            built_engine = builder.build_engine(network, config)
        else:
            builder.max_batch_size = max_batch_size
            builder.max_workspace_size = max_workspace_size
            if mix_precision == 'fp16':
                builder.fp16_mode = True
            if mix_precision == 'int8':
                builder.int8_mode = True
                builder.int8_calibrator = calib
            built_engine = builder.build_cuda_engine(network)
        return built_engine

    @staticmethod
    def _pb_uff_parser(pb_dir,
                       network,
                       input_node_names,
                       input_node_shapes,
                       output_node_names):
        parser = trt.UffParser()
        # parse network
        for input_node_name, input_node_shape in zip(input_node_names, input_node_shapes):
            parser.register_input(input_node_name, input_node_shape)
        for output_node_name in output_node_names:
            parser.register_output(output_node_name)
        uff_buffer = uff.from_tensorflow_frozen_model(frozen_file=pb_dir, output_nodes=output_node_names,
                                                      output_filename='buffer.uff', text=False,
                                                      debug_mode=True)
        parser.parse_buffer(uff_buffer, network)
        os.remove('buffer.uff')
        return network

    @staticmethod
    def build_engine_from_pb_or_onnx(model_file,
                                     input_node_names=None,
                                     input_node_shapes=None,
                                     output_node_names=None,
                                     explicit_batch_dim=False,
                                     max_batch_size=1,
                                     max_workspace_size=1 << 30,
                                     mix_precision='fp16',
                                     logger_level='verbose',
                                     calib=None):
        def _assertion():
            name, model_type = tuple(os.path.splitext(model_file))
            assert model_type in ['.pb', '.onnx'], 'invalid model format:{}/(pb-onnx)'.format(model_type)
            if model_type == '.pb':
                assert input_node_names and input_node_shapes and output_node_names, \
                    'input nodes names/shapes and output names are required for parsing .pb'
            assert mix_precision in ['fp16', 'fp32', 'int8'], 'invalid mix precision"{}/{}'. \
                format(mix_precision, ['fp16', 'fp32', 'int8'])
            if mix_precision == 'int8':
                assert calib is not None, 'calibrator is required for int8 mode'
            valid_logger_level = ['verbose', 'error']
            assert logger_level in valid_logger_level, 'valid log level:{}'.format(valid_logger_level)

        def _trt_logger():
            cmd_str = 'trt_logger = trt.Logger(trt.Logger.{})'.format(logger_level.upper())
            exec(cmd_str)
        _assertion()
        _trt_logger()

        logger.info('building engine from:{}'.format(model_file))
        logger.info('explict batch dim:{}'.format(explicit_batch_dim))
        name, model_type = tuple(os.path.splitext(model_file))

        # force explicit batch dim flag for onnx model, 7.0.0 only supports parsing onnx with explicit_batch flag
        if os.path.splitext(model_file)[-1] == '.onnx':
            explicit_batch_dim = 1
            logger.info('forcing explicit batch flag for onnx model, '
                        '7.0.0 only supports parsing onnx with explicit_batch flag')

        # initialize builder
        builder = trt.Builder(TRT_LOGGER)
        network_flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH) if explicit_batch_dim else 0
        network = builder.create_network(network_flag)
        # parse network
        if model_type == '.pb':
            input_node_names = TensorrtBuilder._item_to_list(input_node_names)
            output_node_names = TensorrtBuilder._item_to_list(output_node_names)
            network = TensorrtBuilder._pb_uff_parser(model_file, network, input_node_names, input_node_shapes,
                                                     output_node_names)
        else:
            parser = trt.OnnxParser(network, TRT_LOGGER)
            with open(model_file, 'rb') as model:
                if not parser.parse(model.read()):
                    logger.error('ERROR: Failed to parse the ONNX file: {}'.format(model_file))
                    for error in range(parser.num_errors):
                        logger.error(parser.get_error(error))
                    sys.exit(1)
        # build engine
        built_engine = TensorrtBuilder._build_engine(network, builder, explicit_batch_dim, max_batch_size,
                                                     max_workspace_size,
                                                     mix_precision, calib)
        if built_engine:
            logger.info('engine built!')
            TensorrtBuilder._save_engine(built_engine, name)
            return built_engine
        else:
            logger.error('fail to build engine!')
            return False


class InferenceWithTensorRT:
    def __init__(self, model_file, pre_processing_fn=None, post_processing_fn=None, force_rebuild=False, **kwargs):
        self.model_dir = model_file
        self.pre_processing_fn = pre_processing_fn
        self.post_processing_fn = post_processing_fn
        self.kwargs = kwargs
        self.force_rebuild = force_rebuild
        self._engine_init()
        self._context_init()

    def _engine_init(self):
        """
        load a engine buffer or buid a new one
        :return: a trt engine obj
        """
        self.trt_runtime = trt.Runtime(TRT_LOGGER)
        self.trt_engine = None
        engine_file = os.path.splitext(self.model_dir)[0] + '.engine'
        if not os.path.exists(engine_file) or self.force_rebuild:
            print('no built engine found, building a new one...')
            model_type = os.path.splitext(self.model_dir)[-1]
            valid_model_format = ['.pb', '.onnx']
            assert model_type in valid_model_format, 'provided model is invalid:{}/{}'.format(model_type,
                                                                                              valid_model_format)
            self.trt_engine = TensorrtBuilder.build_engine_from_pb_or_onnx(self.model_dir, **self.kwargs)
        else:
            print('loading built engine:{}...'.format(engine_file))
            self.trt_engine = TensorrtBuilder._load_engine(self.trt_runtime, engine_file)

    def _context_init(self):
        volume = trt.volume(self.trt_engine.get_binding_shape(0)) * self.trt_engine.max_batch_size
        self.input_dtype = trt.nptype(self.trt_engine.get_binding_dtype(0))
        self.host_input = cuda.pagelocked_empty(volume, dtype=self.input_dtype)
        volume = trt.volume(self.trt_engine.get_binding_shape(1)) * self.trt_engine.max_batch_size
        dtype = trt.nptype(self.trt_engine.get_binding_dtype(1))
        self.host_output = cuda.pagelocked_empty(volume, dtype=dtype)
        # Allocate device memory for inputs and outputs.
        self.cuda_input = cuda.mem_alloc(self.host_input.nbytes)
        self.cuda_output = cuda.mem_alloc(self.host_output.nbytes)
        self.context = self.trt_engine.create_execution_context()
        self.context.active_optimization_profile = 0
        self.stream = cuda.Stream()

    def predict(self, input_data):
        """
        predict with async api
        data -> cpu -> GPU -> cpu
        :param input_data:
        :param kwargs:
        :return:
        """
        if self.pre_processing_fn is not None:
            input_data = self.pre_processing_fn(input_data)
        if str(input_data.dtype) != self.input_dtype.__name__:
            logging.warning('dtype of input data:{} is not compilable with engine input:{}, enforcing dtype convertion'
                            .format(str(input_data.dtype), self.input_dtype.__name__))
            input_data = self.input_dtype(input_data)
        # input data -> cpu
        np.copyto(self.host_input, input_data.ravel())
        # cpu -> gpu
        cuda.memcpy_htod_async(self.cuda_input, self.host_input, self.stream)
        # Run inference. difference execution api by the way the engine built(implicit/explicit batch size)
        if self.trt_engine.has_implicit_batch_dimension:
            self.context.execute_async(bindings=[int(self.cuda_input), int(self.cuda_output)],
                                       stream_handle=self.stream.handle)
        else:
            self.context.execute_async_v2(bindings=[int(self.cuda_input), int(self.cuda_output)],
                                          stream_handle=self.stream.handle)
        # gpu -> cpu.
        cuda.memcpy_dtoh_async(self.host_output, self.cuda_output, self.stream)
        # Synchronize the stream
        self.stream.synchronize()
        output = self.host_output
        if self.post_processing_fn is not None:
            output = self.post_processing_fn(output)
        # Return the host output.
        return output


class CustomEntropyCalibrator(trt.IInt8EntropyCalibrator2):
    """
    simple calibrator passed to builder for building a int8 engine.
    """

    def __init__(self, data_gen,  # a python generator, each yield return a batch of x(N, C)/ (y is not required)
                 cache_file,  # calibrator cache file name, str
                 batch_size=8,
                 input_shape_wo_batch_dim=(4, 1024, 1024)):
        # Whenever you specify a custom constructor for a TensorRT class,
        # you MUST call the constructor of the parent explicitly.
        trt.IInt8EntropyCalibrator2.__init__(self)

        self.cache_file = cache_file
        self.batch_size = batch_size
        self.data_gen = data_gen
        # Allocate enough memory for a whole batch.
        self.device_input = cuda.mem_alloc(np.ones((batch_size, *input_shape_wo_batch_dim), dtype=np.float32).nbytes)
        self.batch_count = 0
        self.input_shape = input_shape_wo_batch_dim

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        try:
            batch_data = next(self.data_gen)
            assert batch_data.shape == (self.batch_size, *self.input_shape), 'date batch size is invalid'
            cuda.memcpy_htod(self.device_input, batch_data.ravel())
            # if self.batch_count % 1 == 0:
            logger.info("Calibrating batch {:}, containing {:} images".format(self.batch_count, self.batch_size))
            self.batch_count += 1
            return [int(self.device_input)]
        except StopIteration:
            return None

    def read_calibration_cache(self):
        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.
        if os.path.exists(self.cache_file):
            logger.info('calibrate cache found')
            with open(self.cache_file, "rb") as f:
                return f.read()
        else:
            logger.info('no calibrate cache found')
            return

    def write_calibration_cache(self, cache):
        with open(self.cache_file, "wb") as f:
            f.write(cache)

```




### [MMN](https://www.yuque.com/mnn/cn/create_session)

阿里家的移动端推理框架

### [NCNN](https://github.com/Tencent/ncnn)

腾讯的移动端推理框架